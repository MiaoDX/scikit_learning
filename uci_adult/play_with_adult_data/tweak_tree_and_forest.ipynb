{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from data_preprocessing import loadData,getScaledAndOneHotEncoderedX,getLineFromFile,simpleScale,testModelOnData,scaleWithFeaturesAndKeepLocation\n",
    "from data_preprocessing import checkNegative\n",
    "from data_preprocessing import decisionTreeDemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomForestDemo(X, y, TX, Ty):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    model = RandomForestClassifier()\n",
    "    testModelOnData(model, X, y, TX, Ty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extraTreeDemo(X, y, TX, Ty):\n",
    "    from sklearn.ensemble import ExtraTreesClassifier\n",
    "    # model = ExtraTreesClassifier(n_estimators=10, max_depth=None,min_samples_split=2, random_state=0)\n",
    "    model = ExtraTreesClassifier(n_estimators=10, max_depth=None,min_samples_split=2,\n",
    "                                 max_features=4, random_state=0)\n",
    "    testModelOnData(model, X, y, TX, Ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32561 15\n",
      "16281 15\n",
      "(32561, 105)\n"
     ]
    }
   ],
   "source": [
    "X, y, TX, Ty = loadData()\n",
    "\n",
    "scaledAndOneHotX = getScaledAndOneHotEncoderedX(X) \n",
    "scaledAndOneHotTX = getScaledAndOneHotEncoderedX(TX) \n",
    "print(scaledAndOneHotX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32561, 14) (32561, 14)\n",
      "[[  3.90000000e+01   7.75160000e+04   1.30000000e+01   2.17400000e+03\n",
      "    0.00000000e+00   4.00000000e+01]\n",
      " [  5.00000000e+01   8.33110000e+04   1.30000000e+01   0.00000000e+00\n",
      "    0.00000000e+00   1.30000000e+01]\n",
      " [  3.80000000e+01   2.15646000e+05   9.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   4.00000000e+01]]\n",
      "[[ 0.03067056 -1.06361075  1.13473876  0.1484529  -0.21665953 -0.03542945]\n",
      " [ 0.83710898 -1.008707    1.13473876 -0.14592048 -0.21665953 -2.22215312]\n",
      " [-0.04264203  0.2450785  -0.42005962 -0.14592048 -0.21665953 -0.03542945]]\n"
     ]
    }
   ],
   "source": [
    "continuous_features = list(map(lambda x: x - 1, [1, 3, 5, 11, 12, 13]))\n",
    "scaledX = scaleWithFeaturesAndKeepLocation(X, continuous_features)\n",
    "scaledTX = scaleWithFeaturesAndKeepLocation(TX, continuous_features)\n",
    "print(X.shape, scaledX.shape)\n",
    "\n",
    "X12 = X[:3]\n",
    "scaledX12 = scaledX[:3]\n",
    "print(X12[:,continuous_features])\n",
    "print(scaledX12[:,continuous_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "[All decision trees use np.float32 arrays internally. If training data is not in this format, a copy of the dataset will be made.](http://scikit-learn.org/stable/modules/tree.html#tips-on-practical-use)\n",
    "Do not see difference.\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "X = X.astype(np.float32)\n",
    "TX = TX.astype(np.float32)\n",
    "scaledX = scaledX.astype(np.float32)\n",
    "scaledTX = scaledTX.astype(np.float32)\n",
    "scaledAndOneHotX = scaledAndOneHotX.astype(np.float32)\n",
    "scaledAndOneHotTX = scaledAndOneHotTX.astype(np.float32)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "feature_names = ['age','workclass','fnlwgt','education','education-num','marital-status','occupation','relationship','race','sex','capital-gain','capital-loss','hours-per-week','native-country']\n",
    "class_names = [\"gt 50K\",\"le 50K\"]\n",
    "print(len(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Usage:\n",
    "    saveTree2DotAndPdf(clf, feature_names=feature_names, class_names=class_names)\n",
    "'''\n",
    "def saveTree2DotAndPdf(clf, feature_names, class_names, file_name='tmp', showImage=False):\n",
    "    from sklearn import tree\n",
    "    dotFile = file_name+\".dot\"\n",
    "    pdfFile = file_name+\".pdf\"\n",
    "    with open(dotFile, 'w') as f:\n",
    "        f = tree.export_graphviz(clf, out_file=f,\n",
    "                            feature_names=feature_names,\n",
    "                            class_names=class_names,\n",
    "                            filled=True, \n",
    "                            rounded=True,  \n",
    "                            special_characters=True\n",
    "                            )\n",
    "    print('save to dot file done\\n')\n",
    "    import pydotplus \n",
    "    # dot_data = tree.export_graphviz(clf, out_file=None) \n",
    "    # graph = pydotplus.graph_from_dot_data(dot_data) \n",
    "    graph = pydotplus.graph_from_dot_file(dotFile)\n",
    "    graph.write_pdf(pdfFile)\n",
    "    print('save to pdf file done\\n')\n",
    "\n",
    "    \n",
    "    if showImage:\n",
    "        print('prepring to show the image, may take a long time...\\n')\n",
    "        from IPython.display import Image\n",
    "        Image(graph.create_png())\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      1.00      1.00      7841\n",
      "        1.0       1.00      1.00      1.00     24720\n",
      "\n",
      "avg / total       1.00      1.00      1.00     32561\n",
      "\n",
      "[[ 7841     0]\n",
      " [    1 24719]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.60      0.62      0.61      3846\n",
      "        1.0       0.88      0.87      0.88     12435\n",
      "\n",
      "avg / total       0.82      0.81      0.81     16281\n",
      "\n",
      "[[ 2372  1474]\n",
      " [ 1562 10873]]\n",
      "\n",
      "\n",
      "[ 0.80818132  0.8133407   0.81258638]\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      1.00      1.00      7841\n",
      "        1.0       1.00      1.00      1.00     24720\n",
      "\n",
      "avg / total       1.00      1.00      1.00     32561\n",
      "\n",
      "[[ 7841     0]\n",
      " [    1 24719]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.60      0.61      0.60      3846\n",
      "        1.0       0.88      0.88      0.88     12435\n",
      "\n",
      "avg / total       0.81      0.81      0.81     16281\n",
      "\n",
      "[[ 2328  1518]\n",
      " [ 1529 10906]]\n",
      "\n",
      "\n",
      "[ 0.81076101  0.81426202  0.81258638]\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      1.00      1.00      7841\n",
      "        1.0       1.00      1.00      1.00     24720\n",
      "\n",
      "avg / total       1.00      1.00      1.00     32561\n",
      "\n",
      "[[ 7841     0]\n",
      " [    1 24719]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.60      0.60      0.60      3846\n",
      "        1.0       0.88      0.87      0.88     12435\n",
      "\n",
      "avg / total       0.81      0.81      0.81     16281\n",
      "\n",
      "[[ 2316  1530]\n",
      " [ 1558 10877]]\n",
      "\n",
      "\n",
      "[ 0.81066888  0.81103741  0.81728554]\n"
     ]
    }
   ],
   "source": [
    "# Default decisionTree\n",
    "decisionTreeDemo(X, y, TX, Ty)\n",
    "decisionTreeDemo(scaledX, y, scaledTX, Ty)\n",
    "decisionTreeDemo(scaledAndOneHotX,y,scaledAndOneHotTX,Ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32561, 14)\n",
      "(32561, 2) (16281, 2)\n",
      "[10 11]\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.97      0.31      0.47      7841\n",
      "        1.0       0.82      1.00      0.90     24720\n",
      "\n",
      "avg / total       0.86      0.83      0.80     32561\n",
      "\n",
      "[[ 2450  5391]\n",
      " [   75 24645]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.97      0.31      0.47      3846\n",
      "        1.0       0.82      1.00      0.90     12435\n",
      "\n",
      "avg / total       0.86      0.83      0.80     16281\n",
      "\n",
      "[[ 1184  2662]\n",
      " [   36 12399]]\n",
      "\n",
      "\n",
      "[ 0.83287267  0.83287267  0.82861882]\n"
     ]
    }
   ],
   "source": [
    "# [1.10.5. Tips on practical use](http://scikit-learn.org/stable/modules/tree.html#tips-on-practical-use)\n",
    "\n",
    "#1 feature selection/dimensionality reduction \n",
    "\n",
    "'''\n",
    "## 1.1 SelectKBest\n",
    "Test different k,found when k=2, we got best scores.\n",
    "The parameters selected is [10,11], aka, 'capital-gain' and 'capital-loss'\n",
    "'''\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest,chi2\n",
    "skb = SelectKBest(chi2, k=2).fit(X, y) \n",
    "\n",
    "X_new = skb.transform(X)\n",
    "TX_new = skb.transform(TX)\n",
    "\n",
    "\n",
    "print(X.shape)\n",
    "print(X_new.shape,TX_new.shape)\n",
    "print(skb.get_support(indices=True))\n",
    "\n",
    "decisionTreeDemo(X_new, y, TX_new, Ty)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter ckeck negative..\n",
      "141817\n",
      "(0, 2)\n",
      "-1.06361\n",
      "(32561, 14)\n",
      "(32561, 2) (16281, 2)\n",
      "[10 11]\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.97      0.31      0.47      7841\n",
      "        1.0       0.82      1.00      0.90     24720\n",
      "\n",
      "avg / total       0.86      0.83      0.80     32561\n",
      "\n",
      "[[ 2450  5391]\n",
      " [   75 24645]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.92      0.24      0.39      3846\n",
      "        1.0       0.81      0.99      0.89     12435\n",
      "\n",
      "avg / total       0.84      0.82      0.77     16281\n",
      "\n",
      "[[  939  2907]\n",
      " [   84 12351]]\n",
      "\n",
      "\n",
      "[ 0.83287267  0.83287267  0.82861882]\n",
      "(32561, 105)\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.93      0.95      0.94      7841\n",
      "        1.0       0.98      0.98      0.98     24720\n",
      "\n",
      "avg / total       0.97      0.97      0.97     32561\n",
      "\n",
      "[[ 7464   377]\n",
      " [  597 24123]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.60      0.63      0.61      3846\n",
      "        1.0       0.88      0.87      0.88     12435\n",
      "\n",
      "avg / total       0.82      0.81      0.81     16281\n",
      "\n",
      "[[ 2405  1441]\n",
      " [ 1609 10826]]\n",
      "\n",
      "\n",
      "[ 0.81665745  0.81453842  0.81571916]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# Select K best with scaled data\n",
    "\n",
    "No imporvement\n",
    "'''\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest,chi2\n",
    "skb = SelectKBest(chi2, k=2).fit(X, y) \n",
    "\n",
    "checkNegative(scaledX)\n",
    "\n",
    "scaledX_new = skb.transform(scaledX)\n",
    "scaledTX_new = skb.transform(scaledTX)\n",
    "\n",
    "\n",
    "print(X.shape)\n",
    "print(scaledX_new.shape,scaledTX_new.shape)\n",
    "print(skb.get_support(indices=True))\n",
    "\n",
    "\n",
    "\n",
    "decisionTreeDemo(scaledX_new, y, scaledTX_new, Ty)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "chi2 failed because of it [Compute chi-squared stats between each non-negative feature and class](https://github.com/scikit-learn/scikit-learn/blob/14031f6/sklearn/feature_selection/univariate_selection.py#L305)\n",
    "'''\n",
    "\n",
    "print(scaledAndOneHotX.shape)\n",
    "# checkNegative(scaledAndOneHotX)\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest,chi2,f_classif\n",
    "\n",
    "skb2 = SelectKBest(f_classif, k=50).fit(scaledAndOneHotX, y)\n",
    "scaledAndOneHotXKBest = skb2.transform(scaledAndOneHotX)\n",
    "scaledAndOneHotTXKBest = skb2.transform(scaledAndOneHotTX)\n",
    "\n",
    "decisionTreeDemo(scaledAndOneHotXKBest, y, scaledAndOneHotTXKBest, Ty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.15265516  0.04189486  0.16356164  0.03056916  0.08820702  0.08197178\n",
      "  0.07991579  0.0711433   0.01323944  0.03709689  0.09349672  0.03161799\n",
      "  0.09370832  0.02092191]\n",
      "(32561, 7) (16281, 7)\n",
      "[  5.00000000e+01   8.33110000e+04   1.30000000e+01   0.00000000e+00\n",
      "   4.00000000e+00   0.00000000e+00   1.30000000e+01]\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      1.00      1.00      7841\n",
      "        1.0       1.00      1.00      1.00     24720\n",
      "\n",
      "avg / total       1.00      1.00      1.00     32561\n",
      "\n",
      "[[ 7841     0]\n",
      " [    4 24716]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.58      0.59      0.58      3846\n",
      "        1.0       0.87      0.87      0.87     12435\n",
      "\n",
      "avg / total       0.80      0.80      0.80     16281\n",
      "\n",
      "[[ 2259  1587]\n",
      " [ 1637 10798]]\n",
      "\n",
      "\n",
      "[ 0.79786254  0.81076101  0.80503087]\n",
      "[ 0.15374158  0.04370093  0.16248965  0.03060041  0.09547375  0.10379454\n",
      "  0.07853315  0.0563238   0.01347666  0.02926605  0.08754354  0.02813995\n",
      "  0.09699471  0.01992128]\n",
      "(32561, 14) (16281, 14) (32561, 7) (16281, 7)\n",
      "[ 0.83710897 -1.00870705  1.1347388   0.          4.         -0.14592049\n",
      " -2.22215319]\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      1.00      1.00      7841\n",
      "        1.0       1.00      1.00      1.00     24720\n",
      "\n",
      "avg / total       1.00      1.00      1.00     32561\n",
      "\n",
      "[[ 7841     0]\n",
      " [    4 24716]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.57      0.57      0.57      3846\n",
      "        1.0       0.87      0.87      0.87     12435\n",
      "\n",
      "avg / total       0.80      0.80      0.80     16281\n",
      "\n",
      "[[ 2199  1647]\n",
      " [ 1640 10795]]\n",
      "\n",
      "\n",
      "[ 0.79758614  0.81186659  0.80337234]\n",
      "(32561, 105) (16281, 105) (32561, 19) (16281, 19)\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      1.00      1.00      7841\n",
      "        1.0       1.00      1.00      1.00     24720\n",
      "\n",
      "avg / total       1.00      1.00      1.00     32561\n",
      "\n",
      "[[ 7841     0]\n",
      " [    9 24711]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.59      0.60      0.59      3846\n",
      "        1.0       0.87      0.87      0.87     12435\n",
      "\n",
      "avg / total       0.81      0.81      0.81     16281\n",
      "\n",
      "[[ 2291  1555]\n",
      " [ 1565 10870]]\n",
      "\n",
      "\n",
      "[ 0.80854984  0.81011609  0.81645628]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "## 1.2 Tree-based feature selection\n",
    "\n",
    "It seems that we found nice attrs to use, but the default tree is not so good.\n",
    "'''\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "clf = ExtraTreesClassifier()\n",
    "clf = clf.fit(X, y)\n",
    "print(clf.feature_importances_)\n",
    "\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "extraTreesX = model.transform(X)\n",
    "extraTreesTX = model.transform(TX)\n",
    "print(extraTreesX.shape, extraTreesTX.shape)\n",
    "print(extraTreesX[1])\n",
    "\n",
    "decisionTreeDemo(extraTreesX, y, extraTreesTX, Ty)\n",
    "\n",
    "\n",
    "\n",
    "# scaled data\n",
    "clf = clf.fit(X, y)\n",
    "print(clf.feature_importances_)\n",
    "\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "extraTreesScaledX = model.transform(scaledX)\n",
    "extraTreesScaledTX = model.transform(scaledTX)\n",
    "print(scaledX.shape, scaledTX.shape, extraTreesScaledX.shape, extraTreesScaledTX.shape)\n",
    "print(extraTreesScaledX[1])\n",
    "\n",
    "decisionTreeDemo(extraTreesScaledX, y, extraTreesScaledTX, Ty)\n",
    "\n",
    "\n",
    "\n",
    "# scaled and onehot encodered data\n",
    "clf.fit(scaledAndOneHotX, y)\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "extraTreeScaledAndOneHotX = model.transform(scaledAndOneHotX)\n",
    "extraTreeScaledAndOneHotTX = model.transform(scaledAndOneHotTX)\n",
    "print(scaledAndOneHotX.shape, scaledAndOneHotTX.shape, extraTreeScaledAndOneHotX.shape, extraTreeScaledAndOneHotTX.shape)\n",
    "\n",
    "decisionTreeDemo(extraTreeScaledAndOneHotX, y, extraTreeScaledAndOneHotTX, Ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32561, 7) (16281, 7)\n",
      "(32561, 7) (16281, 7)\n",
      "(32561, 19) (16281, 19)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "## 1.3 PCA way\n",
    "\n",
    "Use the extraTree got nums as the n_components, aka, X->7, scalaedX->7, scaledAndOneHotX->19\n",
    "\n",
    "params:\n",
    "    >batch_size,The number of samples to use for each batch. \n",
    "     If batch_size is None, then batch_size is inferred from the data and set to 5 * n_features. \n",
    "     \n",
    "     Not suitable for our onehot encodered data.\n",
    "\n",
    "'''\n",
    "\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "\n",
    "n_components = 7\n",
    "ipca = IncrementalPCA(n_components=n_components, batch_size=None)\n",
    "ipca.fit(X, y)\n",
    "pcaX = ipca.transform(X)\n",
    "pcaTX = ipca.transform(TX)\n",
    "\n",
    "print(pcaX.shape, pcaTX.shape)\n",
    "\n",
    "n_components = 7\n",
    "ipca = IncrementalPCA(n_components=n_components, batch_size=None)\n",
    "pcaScaledX = ipca.fit_transform(scaledX)\n",
    "pcaScaledTX = ipca.fit_transform(scaledTX)\n",
    "\n",
    "print(pcaScaledX.shape, pcaScaledTX.shape)\n",
    "\n",
    "\n",
    "n_components = 19\n",
    "ipca2 = IncrementalPCA(n_components=n_components, batch_size=50)\n",
    "pcaScaledAndOneHotX = ipca2.fit_transform(scaledAndOneHotX)\n",
    "pcaScaledAndOneHotTX = ipca2.fit_transform(scaledAndOneHotTX)\n",
    "\n",
    "print(pcaScaledAndOneHotX.shape, pcaScaledAndOneHotTX.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=8,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.75      0.61      0.67      7841\n",
      "        1.0       0.88      0.93      0.91     24720\n",
      "\n",
      "avg / total       0.85      0.86      0.85     32561\n",
      "\n",
      "[[ 4748  3093]\n",
      " [ 1618 23102]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.72      0.57      0.64      3846\n",
      "        1.0       0.88      0.93      0.90     12435\n",
      "\n",
      "avg / total       0.84      0.85      0.84     16281\n",
      "\n",
      "[[ 2202  1644]\n",
      " [  858 11577]]\n",
      "\n",
      "\n",
      "[ 0.84613967  0.8462318   0.85257532]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Tweak the params to get better performance by hand.\n",
    "\n",
    "1.Only change the max_depth, [5-10] gives similar nice results of 0.84, and train/test datasets seems alike.\n",
    "\n",
    "2.[Balance your dataset before training to prevent the tree from being biased toward the classes that are dominant.](http://scikit-learn.org/stable/modules/tree.html#tips-on-practical-use)\n",
    "class_weight=\"balanced\", see [class_weight : dict, list of dicts, “balanced” or None, optional (default=None)](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)\n",
    "     However, not so appealling.\n",
    "\n",
    "3.\n",
    "'''\n",
    "\n",
    "from sklearn import tree\n",
    "model = tree.DecisionTreeClassifier(max_depth=8)\n",
    "# testModelOnData(model, X, y, TX, Ty)\n",
    "\n",
    "# testModelOnData(model, extraTreesX, y, extraTreesTX, Ty)\n",
    "testModelOnData(model, extraTreesScaledX, y, extraTreesScaledTX, Ty)\n",
    "# testModelOnData(model, extraTreeScaledAndOneHotX, y, extraTreeScaledAndOneHotTX, Ty)\n",
    "\n",
    "# testModelOnData(model, pcaX, y, pcaTX, Ty)\n",
    "# testModelOnData(model, pcaScaledX, y, pcaScaledTX, Ty)\n",
    "# testModelOnData(model, pcaScaledAndOneHotX, y, pcaScaledAndOneHotTX, Ty)\n",
    "\n",
    "\n",
    "# saveTree2DotAndPdf(model, feature_names=feature_names, class_names=class_names, file_name='maxdepth4_extraTree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=4, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=20, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      0.99      0.99      7841\n",
      "        1.0       1.00      1.00      1.00     24720\n",
      "\n",
      "avg / total       1.00      1.00      1.00     32561\n",
      "\n",
      "[[ 7795    46]\n",
      " [   36 24684]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.70      0.64      0.67      3846\n",
      "        1.0       0.89      0.91      0.90     12435\n",
      "\n",
      "avg / total       0.84      0.85      0.85     16281\n",
      "\n",
      "[[ 2451  1395]\n",
      " [ 1070 11365]]\n",
      "\n",
      "\n",
      "[ 0.84927216  0.85074627  0.85487884]\n",
      "5\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=5, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=20, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.99      0.99      0.99      7841\n",
      "        1.0       1.00      1.00      1.00     24720\n",
      "\n",
      "avg / total       1.00      1.00      1.00     32561\n",
      "\n",
      "[[ 7788    53]\n",
      " [   57 24663]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.68      0.62      0.65      3846\n",
      "        1.0       0.89      0.91      0.90     12435\n",
      "\n",
      "avg / total       0.84      0.84      0.84     16281\n",
      "\n",
      "[[ 2389  1457]\n",
      " [ 1130 11305]]\n",
      "\n",
      "\n",
      "[ 0.83867699  0.84051962  0.84253202]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "Tweak RandomForest params to get better performance by hand.\n",
    "\n",
    "1.max_features\n",
    "    [1.11.2.3. Parameters](http://scikit-learn.org/stable/modules/ensemble.html#parameters)\n",
    "    max_features=sqrt(n_features) for classification tasks\n",
    "'''\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from math import sqrt,ceil\n",
    "\n",
    "'''\n",
    "# baseline\n",
    "\n",
    "They seems nice, but overfit, should have better results.\n",
    "'''\n",
    "# randomForestDemo(X, y, TX, Ty) # [ 0.84374424  0.84208587  0.84630978]\n",
    "# randomForestDemo(extraTreesX, y, extraTreesTX, Ty) # [ 0.83176709  0.83407039  0.83654289]\n",
    "# randomForestDemo(extraTreesScaledX, y, extraTreesScaledTX, Ty) # [ 0.83103004  0.84051962  0.83165945]\n",
    "# randomForestDemo(extraTreeScaledAndOneHotX, y, extraTreeScaledAndOneHotTX, Ty) # [ 0.82881887  0.83250415  0.83433152]\n",
    "\n",
    "max_features = ceil(sqrt(X.shape[1]))\n",
    "print(max_features)\n",
    "model = RandomForestClassifier(n_estimators=20, max_depth=None,max_features=max_features,\n",
    "    min_samples_split=2, random_state=None)\n",
    "\n",
    "\n",
    "testModelOnData(model, X, y, TX, Ty)\n",
    "# testModelOnData(model, extraTreesX, y, extraTreesTX, Ty)\n",
    "# testModelOnData(model, extraTreesScaledX, y, extraTreesScaledTX, Ty)\n",
    "\n",
    "max_features2 = ceil(sqrt(extraTreeScaledAndOneHotX.shape[1]))\n",
    "print(max_features2)\n",
    "model2 = RandomForestClassifier(n_estimators=20, max_depth=None,max_features=max_features2,\n",
    "    min_samples_split=2, random_state=None)\n",
    "testModelOnData(model2, extraTreeScaledAndOneHotX, y, extraTreeScaledAndOneHotTX, Ty)\n",
    "\n",
    "# testModelOnData(model, pcaX, y, pcaTX, Ty)\n",
    "# testModelOnData(model, pcaScaledX, y, pcaScaledTX, Ty)\n",
    "# testModelOnData(model, pcaScaledAndOneHotX, y, pcaScaledAndOneHotTX, Ty)\n",
    "\n",
    "\n",
    "# saveTree2DotAndPdf(model, feature_names=feature_names, class_names=class_names, file_name='maxdepth4_extraTree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
