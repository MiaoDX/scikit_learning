{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from data_preprocessing import loadData,getScaledAndOneHotEncoderedX,getLineFromFile,simpleScale,testModelOnData,scaleWithFeaturesAndKeepLocation\n",
    "from data_preprocessing import checkNegative\n",
    "from data_preprocessing import decisionTreeDemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomForestDemo(X, y, TX, Ty):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    model = RandomForestClassifier()\n",
    "    testModelOnData(model, X, y, TX, Ty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extraTreeDemo(X, y, TX, Ty):\n",
    "    from sklearn.ensemble import ExtraTreesClassifier\n",
    "    # model = ExtraTreesClassifier(n_estimators=10, max_depth=None,min_samples_split=2, random_state=0)\n",
    "    model = ExtraTreesClassifier(n_estimators=10, max_depth=None,min_samples_split=2,\n",
    "                                 max_features='sqrt', random_state=0)\n",
    "    testModelOnData(model, X, y, TX, Ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32561 15\n",
      "16281 15\n",
      "(32561, 105)\n"
     ]
    }
   ],
   "source": [
    "X, y, TX, Ty = loadData()\n",
    "\n",
    "scaledAndOneHotX = getScaledAndOneHotEncoderedX(X) \n",
    "scaledAndOneHotTX = getScaledAndOneHotEncoderedX(TX) \n",
    "print(scaledAndOneHotX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32561, 14) (32561, 14)\n",
      "[[  3.90000000e+01   7.75160000e+04   1.30000000e+01   2.17400000e+03\n",
      "    0.00000000e+00   4.00000000e+01]\n",
      " [  5.00000000e+01   8.33110000e+04   1.30000000e+01   0.00000000e+00\n",
      "    0.00000000e+00   1.30000000e+01]\n",
      " [  3.80000000e+01   2.15646000e+05   9.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   4.00000000e+01]]\n",
      "[[ 0.03067056 -1.06361075  1.13473876  0.1484529  -0.21665953 -0.03542945]\n",
      " [ 0.83710898 -1.008707    1.13473876 -0.14592048 -0.21665953 -2.22215312]\n",
      " [-0.04264203  0.2450785  -0.42005962 -0.14592048 -0.21665953 -0.03542945]]\n"
     ]
    }
   ],
   "source": [
    "continuous_features = list(map(lambda x: x - 1, [1, 3, 5, 11, 12, 13]))\n",
    "scaledX = scaleWithFeaturesAndKeepLocation(X, continuous_features)\n",
    "scaledTX = scaleWithFeaturesAndKeepLocation(TX, continuous_features)\n",
    "print(X.shape, scaledX.shape)\n",
    "\n",
    "X12 = X[:3]\n",
    "scaledX12 = scaledX[:3]\n",
    "print(X12[:,continuous_features])\n",
    "print(scaledX12[:,continuous_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "[All decision trees use np.float32 arrays internally. If training data is not in this format, a copy of the dataset will be made.](http://scikit-learn.org/stable/modules/tree.html#tips-on-practical-use)\n",
    "Do not see difference.\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "X = X.astype(np.float32)\n",
    "TX = TX.astype(np.float32)\n",
    "scaledX = scaledX.astype(np.float32)\n",
    "scaledTX = scaledTX.astype(np.float32)\n",
    "scaledAndOneHotX = scaledAndOneHotX.astype(np.float32)\n",
    "scaledAndOneHotTX = scaledAndOneHotTX.astype(np.float32)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "feature_names = ['age','workclass','fnlwgt','education','education-num','marital-status','occupation','relationship','race','sex','capital-gain','capital-loss','hours-per-week','native-country']\n",
    "class_names = [\"gt 50K\",\"le 50K\"]\n",
    "print(len(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Usage:\n",
    "    saveTree2DotAndPdf(clf, feature_names=feature_names, class_names=class_names)\n",
    "'''\n",
    "def saveTree2DotAndPdf(clf, feature_names, class_names, file_name='tmp', showImage=False):\n",
    "    from sklearn import tree\n",
    "    dotFile = file_name+\".dot\"\n",
    "    pdfFile = file_name+\".pdf\"\n",
    "    with open(dotFile, 'w') as f:\n",
    "        f = tree.export_graphviz(clf, out_file=f,\n",
    "                            feature_names=feature_names,\n",
    "                            class_names=class_names,\n",
    "                            filled=True, \n",
    "                            rounded=True,  \n",
    "                            special_characters=True\n",
    "                            )\n",
    "    print('save to dot file done\\n')\n",
    "    import pydotplus \n",
    "    # dot_data = tree.export_graphviz(clf, out_file=None) \n",
    "    # graph = pydotplus.graph_from_dot_data(dot_data) \n",
    "    graph = pydotplus.graph_from_dot_file(dotFile)\n",
    "    graph.write_pdf(pdfFile)\n",
    "    print('save to pdf file done\\n')\n",
    "\n",
    "    \n",
    "    if showImage:\n",
    "        print('prepring to show the image, may take a long time...\\n')\n",
    "        from IPython.display import Image\n",
    "        Image(graph.create_png())\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      1.00      1.00      7841\n",
      "        1.0       1.00      1.00      1.00     24720\n",
      "\n",
      "avg / total       1.00      1.00      1.00     32561\n",
      "\n",
      "[[ 7841     0]\n",
      " [    1 24719]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.60      0.62      0.61      3846\n",
      "        1.0       0.88      0.87      0.88     12435\n",
      "\n",
      "avg / total       0.82      0.81      0.81     16281\n",
      "\n",
      "[[ 2372  1474]\n",
      " [ 1562 10873]]\n",
      "\n",
      "\n",
      "[ 0.80818132  0.8133407   0.81258638]\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      1.00      1.00      7841\n",
      "        1.0       1.00      1.00      1.00     24720\n",
      "\n",
      "avg / total       1.00      1.00      1.00     32561\n",
      "\n",
      "[[ 7841     0]\n",
      " [    1 24719]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.60      0.61      0.60      3846\n",
      "        1.0       0.88      0.88      0.88     12435\n",
      "\n",
      "avg / total       0.81      0.81      0.81     16281\n",
      "\n",
      "[[ 2328  1518]\n",
      " [ 1529 10906]]\n",
      "\n",
      "\n",
      "[ 0.81076101  0.81426202  0.81258638]\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      1.00      1.00      7841\n",
      "        1.0       1.00      1.00      1.00     24720\n",
      "\n",
      "avg / total       1.00      1.00      1.00     32561\n",
      "\n",
      "[[ 7841     0]\n",
      " [    1 24719]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.60      0.60      0.60      3846\n",
      "        1.0       0.88      0.87      0.88     12435\n",
      "\n",
      "avg / total       0.81      0.81      0.81     16281\n",
      "\n",
      "[[ 2316  1530]\n",
      " [ 1558 10877]]\n",
      "\n",
      "\n",
      "[ 0.81066888  0.81103741  0.81728554]\n"
     ]
    }
   ],
   "source": [
    "# Default decisionTree\n",
    "decisionTreeDemo(X, y, TX, Ty)\n",
    "decisionTreeDemo(scaledX, y, scaledTX, Ty)\n",
    "decisionTreeDemo(scaledAndOneHotX,y,scaledAndOneHotTX,Ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32561, 14)\n",
      "(32561, 2) (16281, 2)\n",
      "[10 11]\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.97      0.31      0.47      7841\n",
      "        1.0       0.82      1.00      0.90     24720\n",
      "\n",
      "avg / total       0.86      0.83      0.80     32561\n",
      "\n",
      "[[ 2450  5391]\n",
      " [   75 24645]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.97      0.31      0.47      3846\n",
      "        1.0       0.82      1.00      0.90     12435\n",
      "\n",
      "avg / total       0.86      0.83      0.80     16281\n",
      "\n",
      "[[ 1184  2662]\n",
      " [   36 12399]]\n",
      "\n",
      "\n",
      "[ 0.83287267  0.83287267  0.82861882]\n"
     ]
    }
   ],
   "source": [
    "# [1.10.5. Tips on practical use](http://scikit-learn.org/stable/modules/tree.html#tips-on-practical-use)\n",
    "\n",
    "#1 feature selection/dimensionality reduction \n",
    "\n",
    "'''\n",
    "## 1.1 SelectKBest\n",
    "Test different k,found when k=2, we got best scores.\n",
    "The parameters selected is [10,11], aka, 'capital-gain' and 'capital-loss'\n",
    "'''\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest,chi2\n",
    "skb = SelectKBest(chi2, k=2).fit(X, y) \n",
    "\n",
    "X_new = skb.transform(X)\n",
    "TX_new = skb.transform(TX)\n",
    "\n",
    "\n",
    "print(X.shape)\n",
    "print(X_new.shape,TX_new.shape)\n",
    "print(skb.get_support(indices=True))\n",
    "\n",
    "decisionTreeDemo(X_new, y, TX_new, Ty)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter ckeck negative..\n",
      "141817\n",
      "(0, 2)\n",
      "-1.06361\n",
      "(32561, 14)\n",
      "(32561, 2) (16281, 2)\n",
      "[10 11]\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.97      0.31      0.47      7841\n",
      "        1.0       0.82      1.00      0.90     24720\n",
      "\n",
      "avg / total       0.86      0.83      0.80     32561\n",
      "\n",
      "[[ 2450  5391]\n",
      " [   75 24645]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.92      0.24      0.39      3846\n",
      "        1.0       0.81      0.99      0.89     12435\n",
      "\n",
      "avg / total       0.84      0.82      0.77     16281\n",
      "\n",
      "[[  939  2907]\n",
      " [   84 12351]]\n",
      "\n",
      "\n",
      "[ 0.83287267  0.83287267  0.82861882]\n",
      "(32561, 105)\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.93      0.95      0.94      7841\n",
      "        1.0       0.98      0.98      0.98     24720\n",
      "\n",
      "avg / total       0.97      0.97      0.97     32561\n",
      "\n",
      "[[ 7464   377]\n",
      " [  597 24123]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.60      0.63      0.61      3846\n",
      "        1.0       0.88      0.87      0.88     12435\n",
      "\n",
      "avg / total       0.82      0.81      0.81     16281\n",
      "\n",
      "[[ 2405  1441]\n",
      " [ 1609 10826]]\n",
      "\n",
      "\n",
      "[ 0.81665745  0.81453842  0.81571916]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# Select K best with scaled data\n",
    "\n",
    "No imporvement\n",
    "'''\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest,chi2\n",
    "skb = SelectKBest(chi2, k=2).fit(X, y) \n",
    "\n",
    "checkNegative(scaledX)\n",
    "\n",
    "scaledX_new = skb.transform(scaledX)\n",
    "scaledTX_new = skb.transform(scaledTX)\n",
    "\n",
    "\n",
    "print(X.shape)\n",
    "print(scaledX_new.shape,scaledTX_new.shape)\n",
    "print(skb.get_support(indices=True))\n",
    "\n",
    "\n",
    "\n",
    "decisionTreeDemo(scaledX_new, y, scaledTX_new, Ty)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "chi2 failed because of it [Compute chi-squared stats between each non-negative feature and class](https://github.com/scikit-learn/scikit-learn/blob/14031f6/sklearn/feature_selection/univariate_selection.py#L305)\n",
    "'''\n",
    "\n",
    "print(scaledAndOneHotX.shape)\n",
    "# checkNegative(scaledAndOneHotX)\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest,chi2,f_classif\n",
    "\n",
    "skb2 = SelectKBest(f_classif, k=50).fit(scaledAndOneHotX, y)\n",
    "scaledAndOneHotXKBest = skb2.transform(scaledAndOneHotX)\n",
    "scaledAndOneHotTXKBest = skb2.transform(scaledAndOneHotTX)\n",
    "\n",
    "decisionTreeDemo(scaledAndOneHotXKBest, y, scaledAndOneHotTXKBest, Ty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.15265516  0.04189486  0.16356164  0.03056916  0.08820702  0.08197178\n",
      "  0.07991579  0.0711433   0.01323944  0.03709689  0.09349672  0.03161799\n",
      "  0.09370832  0.02092191]\n",
      "(32561, 7) (16281, 7)\n",
      "[  5.00000000e+01   8.33110000e+04   1.30000000e+01   0.00000000e+00\n",
      "   4.00000000e+00   0.00000000e+00   1.30000000e+01]\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      1.00      1.00      7841\n",
      "        1.0       1.00      1.00      1.00     24720\n",
      "\n",
      "avg / total       1.00      1.00      1.00     32561\n",
      "\n",
      "[[ 7841     0]\n",
      " [    4 24716]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.58      0.59      0.58      3846\n",
      "        1.0       0.87      0.87      0.87     12435\n",
      "\n",
      "avg / total       0.80      0.80      0.80     16281\n",
      "\n",
      "[[ 2259  1587]\n",
      " [ 1637 10798]]\n",
      "\n",
      "\n",
      "[ 0.79786254  0.81076101  0.80503087]\n",
      "[ 0.15374158  0.04370093  0.16248965  0.03060041  0.09547375  0.10379454\n",
      "  0.07853315  0.0563238   0.01347666  0.02926605  0.08754354  0.02813995\n",
      "  0.09699471  0.01992128]\n",
      "(32561, 14) (16281, 14) (32561, 7) (16281, 7)\n",
      "[ 0.83710897 -1.00870705  1.1347388   0.          4.         -0.14592049\n",
      " -2.22215319]\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      1.00      1.00      7841\n",
      "        1.0       1.00      1.00      1.00     24720\n",
      "\n",
      "avg / total       1.00      1.00      1.00     32561\n",
      "\n",
      "[[ 7841     0]\n",
      " [    4 24716]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.57      0.57      0.57      3846\n",
      "        1.0       0.87      0.87      0.87     12435\n",
      "\n",
      "avg / total       0.80      0.80      0.80     16281\n",
      "\n",
      "[[ 2199  1647]\n",
      " [ 1640 10795]]\n",
      "\n",
      "\n",
      "[ 0.79758614  0.81186659  0.80337234]\n",
      "(32561, 105) (16281, 105) (32561, 19) (16281, 19)\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      1.00      1.00      7841\n",
      "        1.0       1.00      1.00      1.00     24720\n",
      "\n",
      "avg / total       1.00      1.00      1.00     32561\n",
      "\n",
      "[[ 7841     0]\n",
      " [    9 24711]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.59      0.60      0.59      3846\n",
      "        1.0       0.87      0.87      0.87     12435\n",
      "\n",
      "avg / total       0.81      0.81      0.81     16281\n",
      "\n",
      "[[ 2291  1555]\n",
      " [ 1565 10870]]\n",
      "\n",
      "\n",
      "[ 0.80854984  0.81011609  0.81645628]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "## 1.2 Tree-based feature selection\n",
    "\n",
    "It seems that we found nice attrs to use, but the default tree is not so good.\n",
    "'''\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "clf = ExtraTreesClassifier()\n",
    "clf = clf.fit(X, y)\n",
    "print(clf.feature_importances_)\n",
    "\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "extraTreesX = model.transform(X)\n",
    "extraTreesTX = model.transform(TX)\n",
    "print(extraTreesX.shape, extraTreesTX.shape)\n",
    "print(extraTreesX[1])\n",
    "\n",
    "decisionTreeDemo(extraTreesX, y, extraTreesTX, Ty)\n",
    "\n",
    "\n",
    "\n",
    "# scaled data\n",
    "clf = clf.fit(X, y)\n",
    "print(clf.feature_importances_)\n",
    "\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "extraTreesScaledX = model.transform(scaledX)\n",
    "extraTreesScaledTX = model.transform(scaledTX)\n",
    "print(scaledX.shape, scaledTX.shape, extraTreesScaledX.shape, extraTreesScaledTX.shape)\n",
    "print(extraTreesScaledX[1])\n",
    "\n",
    "decisionTreeDemo(extraTreesScaledX, y, extraTreesScaledTX, Ty)\n",
    "\n",
    "\n",
    "\n",
    "# scaled and onehot encodered data\n",
    "clf.fit(scaledAndOneHotX, y)\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "extraTreeScaledAndOneHotX = model.transform(scaledAndOneHotX)\n",
    "extraTreeScaledAndOneHotTX = model.transform(scaledAndOneHotTX)\n",
    "print(scaledAndOneHotX.shape, scaledAndOneHotTX.shape, extraTreeScaledAndOneHotX.shape, extraTreeScaledAndOneHotTX.shape)\n",
    "\n",
    "decisionTreeDemo(extraTreeScaledAndOneHotX, y, extraTreeScaledAndOneHotTX, Ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32561, 7) (16281, 7)\n",
      "(32561, 7) (16281, 7)\n",
      "(32561, 19) (16281, 19)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "## 1.3 PCA way\n",
    "\n",
    "Use the extraTree got nums as the n_components, aka, X->7, scalaedX->7, scaledAndOneHotX->19\n",
    "\n",
    "params:\n",
    "    >batch_size,The number of samples to use for each batch. \n",
    "     If batch_size is None, then batch_size is inferred from the data and set to 5 * n_features. \n",
    "     \n",
    "     Not suitable for our onehot encodered data.\n",
    "\n",
    "'''\n",
    "\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "\n",
    "n_components = 7\n",
    "ipca = IncrementalPCA(n_components=n_components, batch_size=None)\n",
    "ipca.fit(X, y)\n",
    "pcaX = ipca.transform(X)\n",
    "pcaTX = ipca.transform(TX)\n",
    "\n",
    "print(pcaX.shape, pcaTX.shape)\n",
    "\n",
    "n_components = 7\n",
    "ipca = IncrementalPCA(n_components=n_components, batch_size=None)\n",
    "pcaScaledX = ipca.fit_transform(scaledX)\n",
    "pcaScaledTX = ipca.fit_transform(scaledTX)\n",
    "\n",
    "print(pcaScaledX.shape, pcaScaledTX.shape)\n",
    "\n",
    "\n",
    "n_components = 19\n",
    "ipca2 = IncrementalPCA(n_components=n_components, batch_size=50)\n",
    "pcaScaledAndOneHotX = ipca2.fit_transform(scaledAndOneHotX)\n",
    "pcaScaledAndOneHotTX = ipca2.fit_transform(scaledAndOneHotTX)\n",
    "\n",
    "print(pcaScaledAndOneHotX.shape, pcaScaledAndOneHotTX.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
      "            max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.80      0.56      0.66      7841\n",
      "        1.0       0.87      0.96      0.91     24720\n",
      "\n",
      "avg / total       0.86      0.86      0.85     32561\n",
      "\n",
      "[[ 4357  3484]\n",
      " [ 1071 23649]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.76      0.52      0.62      3846\n",
      "        1.0       0.87      0.95      0.90     12435\n",
      "\n",
      "avg / total       0.84      0.85      0.84     16281\n",
      "\n",
      "[[ 2013  1833]\n",
      " [  644 11791]]\n",
      "\n",
      "\n",
      "[ 0.8398747   0.84457343  0.84594121]\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
      "            max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.79      0.56      0.66      7841\n",
      "        1.0       0.87      0.95      0.91     24720\n",
      "\n",
      "avg / total       0.85      0.86      0.85     32561\n",
      "\n",
      "[[ 4406  3435]\n",
      " [ 1204 23516]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.74      0.53      0.62      3846\n",
      "        1.0       0.87      0.94      0.90     12435\n",
      "\n",
      "avg / total       0.84      0.85      0.84     16281\n",
      "\n",
      "[[ 2030  1816]\n",
      " [  702 11733]]\n",
      "\n",
      "\n",
      "[ 0.842178    0.8444813   0.84815258]\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
      "            max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.81      0.53      0.64      7841\n",
      "        1.0       0.86      0.96      0.91     24720\n",
      "\n",
      "avg / total       0.85      0.86      0.84     32561\n",
      "\n",
      "[[ 4132  3709]\n",
      " [  982 23738]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.76      0.48      0.59      3846\n",
      "        1.0       0.86      0.95      0.90     12435\n",
      "\n",
      "avg / total       0.83      0.84      0.83     16281\n",
      "\n",
      "[[ 1856  1990]\n",
      " [  575 11860]]\n",
      "\n",
      "\n",
      "[ 0.84116455  0.83996683  0.8470469 ]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Tweak the params to get better performance by hand.\n",
    "\n",
    "1.Only change the max_depth, [5-10] gives similar nice results of 0.84, and train/test datasets seems alike.\n",
    "\n",
    "2.[Balance your dataset before training to prevent the tree from being biased toward the classes that are dominant.](http://scikit-learn.org/stable/modules/tree.html#tips-on-practical-use)\n",
    "class_weight=\"balanced\", see [class_weight : dict, list of dicts, “balanced” or None, optional (default=None)](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)\n",
    "     However, not so appealling.\n",
    "\n",
    "3.\n",
    "'''\n",
    "\n",
    "from sklearn import tree\n",
    "max_features = ceil(sqrt(X.shape[1]))\n",
    "print(max_features)\n",
    "\n",
    "model = tree.DecisionTreeClassifier(max_depth=10,max_features='sqrt')\n",
    "testModelOnData(model, X, y, TX, Ty)\n",
    "testModelOnData(model, extraTreesX, y, extraTreesTX, Ty)\n",
    "testModelOnData(model, extraTreesScaledX, y, extraTreesScaledTX, Ty)\n",
    "# testModelOnData(model, extraTreeScaledAndOneHotX, y, extraTreeScaledAndOneHotTX, Ty)\n",
    "\n",
    "# testModelOnData(model, pcaX, y, pcaTX, Ty)\n",
    "# testModelOnData(model, pcaScaledX, y, pcaScaledTX, Ty)\n",
    "# testModelOnData(model, pcaScaledAndOneHotX, y, pcaScaledAndOneHotTX, Ty)\n",
    "\n",
    "\n",
    "# saveTree2DotAndPdf(model, feature_names=feature_names, class_names=class_names, file_name='maxdepth4_extraTree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "(32561, 19) 5\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=10, max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=50, n_jobs=-1, oob_score=True, random_state=0,\n",
      "            verbose=0, warm_start=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.82      0.58      0.68      7841\n",
      "        1.0       0.88      0.96      0.92     24720\n",
      "\n",
      "avg / total       0.86      0.87      0.86     32561\n",
      "\n",
      "[[ 4561  3280]\n",
      " [ 1003 23717]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.78      0.54      0.64      3846\n",
      "        1.0       0.87      0.95      0.91     12435\n",
      "\n",
      "avg / total       0.85      0.86      0.85     16281\n",
      "\n",
      "[[ 2093  1753]\n",
      " [  576 11859]]\n",
      "\n",
      "\n",
      "[ 0.85507647  0.86051225  0.86059154]\n",
      "(32561, 7) (32561, 7) (32561, 19)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "Tweak RandomForest params to get better performance by hand.\n",
    "[1.11.2.3. Parameters](http://scikit-learn.org/stable/modules/ensemble.html#parameters)\n",
    "\n",
    "1.n_estimators\n",
    "    the number of trees in the forest. The larger the better, but also the longer it will take to compute. \n",
    "\n",
    "2.max_features    \n",
    "    max_features=sqrt(n_features) for classification tasks\n",
    "\n",
    "[3.2.4.3.1. sklearn.ensemble.RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier)\n",
    "\n",
    "3.oob_score\n",
    "    Whether to use out-of-bag samples to estimate the generalization accuracy.\n",
    "\n",
    "4.n_jobs\n",
    "    If -1, then the number of jobs is set to the number of cores.\n",
    "    \n",
    "5.random_state\n",
    "    const for repeat results\n",
    "'''\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from math import sqrt,ceil\n",
    "\n",
    "'''\n",
    "# baseline\n",
    "\n",
    "They seems nice, but overfit, should have better results.\n",
    "'''\n",
    "# randomForestDemo(X, y, TX, Ty) # [ 0.84374424  0.84208587  0.84630978]\n",
    "# randomForestDemo(extraTreesX, y, extraTreesTX, Ty) # [ 0.83176709  0.83407039  0.83654289]\n",
    "# randomForestDemo(extraTreesScaledX, y, extraTreesScaledTX, Ty) # [ 0.83103004  0.84051962  0.83165945]\n",
    "# randomForestDemo(extraTreeScaledAndOneHotX, y, extraTreeScaledAndOneHotTX, Ty) # [ 0.82881887  0.83250415  0.83433152]\n",
    "\n",
    "max_features = ceil(sqrt(X.shape[1]))\n",
    "print(max_features)\n",
    "model = RandomForestClassifier(n_estimators=50, max_depth=10,max_features='sqrt',oob_score=True,#lass_weight='balanced',\n",
    "    min_samples_split=2, random_state=0,n_jobs=-1)\n",
    "\n",
    "'''\n",
    "Results report, orginal data and extraTreeScaledAndOneHotX:\n",
    "\n",
    "[50,10,max_features]\n",
    "    [ 0.85535287  0.86060439  0.86114438]\n",
    "    [ 0.85562926  0.86051225  0.8609601 ]\n",
    "\n",
    "[50,10,max_features,oob_score]\n",
    "    [ 0.85535287  0.86060439  0.86114438]\n",
    "    [ 0.8551686   0.86189423  0.86197365]\n",
    "    \n",
    "class_weight='balanced' -> still worse\n",
    "    [ 0.80974756  0.80615441  0.81258638]\n",
    "    [ 0.80864198  0.80421964  0.8128628 ]\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "# testModelOnData(model, X, y, TX, Ty)\n",
    "# testModelOnData(model, extraTreesX, y, extraTreesTX, Ty)\n",
    "# testModelOnData(model, extraTreesScaledX, y, extraTreesScaledTX, Ty)\n",
    "\n",
    "\n",
    "max_features2 = ceil(sqrt(extraTreeScaledAndOneHotX.shape[1]))\n",
    "print(extraTreeScaledAndOneHotX.shape,max_features2)\n",
    "testModelOnData(model, extraTreeScaledAndOneHotX, y, extraTreeScaledAndOneHotTX, Ty)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "The pca results are all 0.83**, not so appealing\n",
    "'''\n",
    "print(pcaX.shape, pcaScaledX.shape, pcaScaledAndOneHotX.shape)\n",
    "\n",
    "# testModelOnData(model, pcaX, y, pcaTX, Ty)\n",
    "# testModelOnData(model, pcaScaledX, y, pcaScaledTX, Ty)\n",
    "# testModelOnData(model2, pcaScaledAndOneHotX, y, pcaScaledAndOneHotTX, Ty)\n",
    "\n",
    "\n",
    "# saveTree2DotAndPdf(model, feature_names=feature_names, class_names=class_names, file_name='maxdepth4_extraTree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=5, error_score='raise',\n",
      "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False),\n",
      "       fit_params={}, iid=True, n_jobs=-1,\n",
      "       param_grid={'n_estimators': [50, 70, 90, 110], 'oob_score': [True, False], 'max_depth': [5, 7, 9, 11, 13], 'min_samples_split': [2, 3, 4], 'max_features': ['sqrt', 'log2', None]},\n",
      "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
      "       scoring=None, verbose=0)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.83      0.65      0.73      7841\n",
      "        1.0       0.90      0.96      0.93     24720\n",
      "\n",
      "avg / total       0.88      0.88      0.88     32561\n",
      "\n",
      "[[ 5070  2771]\n",
      " [ 1047 23673]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.78      0.60      0.67      3846\n",
      "        1.0       0.88      0.95      0.91     12435\n",
      "\n",
      "avg / total       0.86      0.86      0.86     16281\n",
      "\n",
      "[[ 2292  1554]\n",
      " [  654 11781]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Tuning RandomForest by GridSearchCV, this should get better results than hand chosen.\n",
    "\n",
    "[3.2. Tuning the hyper-parameters of an estimator](http://scikit-learn.org/stable/modules/grid_search.html#tuning-the-hyper-parameters-of-an-estimator)\n",
    "\n",
    "\n",
    "The GridSearchCV instance implements the usual estimator API: when “fitting” it on a dataset all the possible combinations of parameter values are evaluated and the best combination is retained.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# model = RandomForestClassifier(n_estimators=50, max_depth=10,max_features='sqrt',oob_score=True,#lass_weight='balanced',\n",
    "#     min_samples_split=2, random_state=0,n_jobs=-1)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "'''\n",
    "model.get_params()\n",
    "\n",
    "{'min_samples_leaf': 1, 'max_features': 'auto', 'n_jobs': 1, 'class_weight': None, 'bootstrap': True, 'warm_start': False,\n",
    "    'min_weight_fraction_leaf': 0.0, 'random_state': None, 'verbose': 0, 'criterion': 'gini', 'oob_score': False, \n",
    "    'min_impurity_split': 1e-07, 'min_samples_split': 2, 'n_estimators': 10, 'max_depth': None, 'max_leaf_nodes': None}\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "params = {'n_estimators':[50,70,90,110], 'max_features':['sqrt','log2',None],'oob_score':[True,False],'max_depth':[5,7,9,11,13],\n",
    "         'min_samples_split':[2,3,4]}\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=params, cv=5,\n",
    "                   n_jobs=-1)\n",
    "\n",
    "testModelOnData(grid, X, y, TX, Ty)\n",
    "\n",
    "'''\n",
    "Failed to get all results\n",
    "\n",
    "\n",
    "GridSearchCV(cv=5, error_score='raise',\n",
    "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
    "            verbose=0, warm_start=False),\n",
    "       fit_params={}, iid=True, n_jobs=-1,\n",
    "       param_grid={'n_estimators': [50, 70, 90, 110], 'oob_score': [True, False], 'max_depth': [5, 7, 9, 11, 13], 'min_samples_split': [2, 3, 4], 'max_features': ['sqrt', 'log2', None]},\n",
    "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
    "       scoring=None, verbose=0)\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "        0.0       0.83      0.65      0.73      7841\n",
    "        1.0       0.90      0.96      0.93     24720\n",
    "\n",
    "avg / total       0.88      0.88      0.88     32561\n",
    "\n",
    "[[ 5070  2771]\n",
    " [ 1047 23673]]\n",
    "\n",
    "\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "        0.0       0.78      0.60      0.67      3846\n",
    "        1.0       0.88      0.95      0.91     12435\n",
    "\n",
    "avg / total       0.86      0.86      0.86     16281\n",
    "\n",
    "[[ 2292  1554]\n",
    " [  654 11781]]\n",
    "'''\n",
    "\n",
    "\n",
    "# testModelOnData(model, extraTreesX, y, extraTreesTX, Ty)\n",
    "# testModelOnData(model, extraTreesScaledX, y, extraTreesScaledTX, Ty)\n",
    "# testModelOnData(model, extraTreeScaledAndOneHotX, y, extraTreeScaledAndOneHotTX, Ty)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "http://scikit-learn.org/stable/modules/grid_search.html#specifying-an-objective-metric\n",
    "     For some applications, other scoring functions are better suited (for example in unbalanced classification, the accuracy score is often uninformative).\n",
    "\n",
    "So we use `f1` for alternative\n",
    "\n",
    "\n",
    "refit : boolean, default=True\n",
    "\n",
    "    Refit the best estimator with the entire dataset. If “False”, it is impossible to make predictions using this GridSearchCV instance after fitting.\n",
    "\n",
    "'''\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# model = RandomForestClassifier(n_estimators=50, max_depth=10,max_features='sqrt',oob_score=True,#lass_weight='balanced',\n",
    "#     min_samples_split=2, random_state=0,n_jobs=-1)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "'''\n",
    "model.get_params()\n",
    "\n",
    "{'min_samples_leaf': 1, 'max_features': 'auto', 'n_jobs': 1, 'class_weight': None, 'bootstrap': True, 'warm_start': False,\n",
    "    'min_weight_fraction_leaf': 0.0, 'random_state': None, 'verbose': 0, 'criterion': 'gini', 'oob_score': False, \n",
    "    'min_impurity_split': 1e-07, 'min_samples_split': 2, 'n_estimators': 10, 'max_depth': None, 'max_leaf_nodes': None}\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "params = {'n_estimators':[50,110], 'max_features':['sqrt','log2'],'oob_score':[True],'max_depth':[5,9,11],\n",
    "         'min_samples_split':[2,4]}\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=params, cv=5,\n",
    "                    scoring='f1', #see above\n",
    "                    error_score=0, # to avoid crash\n",
    "                   n_jobs=-1)\n",
    "\n",
    "testModelOnData(grid, X, y, TX, Ty)\n",
    "# testModelOnData(model, extraTreesX, y, extraTreesTX, Ty)\n",
    "# testModelOnData(model, extraTreesScaledX, y, extraTreesScaledTX, Ty)\n",
    "# testModelOnData(model, extraTreeScaledAndOneHotX, y, extraTreeScaledAndOneHotTX, Ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=0.1, n_estimators=10000, random_state=0)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.79      0.64      0.71      7841\n",
      "        1.0       0.89      0.95      0.92     24720\n",
      "\n",
      "avg / total       0.87      0.87      0.87     32561\n",
      "\n",
      "[[ 5027  2814]\n",
      " [ 1330 23390]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.77      0.64      0.70      3846\n",
      "        1.0       0.89      0.94      0.92     12435\n",
      "\n",
      "avg / total       0.87      0.87      0.87     16281\n",
      "\n",
      "[[ 2444  1402]\n",
      " [  710 11725]]\n",
      "\n",
      "\n",
      "[ 0.86668509  0.8684356   0.87275408]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "We should think the best results can be \n",
    "    For learning rate 0.1\n",
    "        n_estimators=10000\n",
    "        \n",
    "        [ 0.86668509  0.8684356   0.87275408]\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "Adaboosting TRY\n",
    "\n",
    "\n",
    ">An AdaBoost [1] classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional \n",
    "copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent \n",
    "classifiers focus more on difficult cases\n",
    "--http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier\n",
    "\n",
    "\n",
    "\n",
    "It seems nice, sine the defalt have \n",
    "[ 0.85701124  0.85866961  0.86252649] on orginal data.\n",
    "\n",
    "params:\n",
    "1.learning rate\n",
    "    Learning rate shrinks the contribution of each classifier by learning_rate. There is a trade-off between learning_rate and n_estimators\n",
    "    \n",
    "    For learning rate 0.5,\n",
    "        n_estimators=500\n",
    "        [ 0.8641054   0.86825134  0.86980558]\n",
    "        \n",
    "        1000\n",
    "        [ 0.86557951  0.86797494  0.87229338]\n",
    "    \n",
    "    For learning rate 0.1\n",
    "        n_estimators=10000\n",
    "        \n",
    "        [ 0.86668509  0.8684356   0.87275408]        \n",
    "\n",
    "'''\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "model = AdaBoostClassifier(n_estimators=10000, learning_rate=0.1,\n",
    "                           random_state=0)\n",
    "\n",
    "testModelOnData(model, X, y, TX, Ty)\n",
    "# testModelOnData(model, extraTreesX, y, extraTreesTX, Ty)\n",
    "# testModelOnData(model, extraTreesScaledX, y, extraTreesScaledTX, Ty)\n",
    "# testModelOnData(model, extraTreeScaledAndOneHotX, y, extraTreeScaledAndOneHotTX, Ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features='sqrt', max_leaf_nodes=None,\n",
      "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "              n_estimators=2000, presort='auto', random_state=0,\n",
      "              subsample=1.0, verbose=0, warm_start=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.84      0.72      0.77      7841\n",
      "        1.0       0.91      0.96      0.93     24720\n",
      "\n",
      "avg / total       0.90      0.90      0.90     32561\n",
      "\n",
      "[[ 5631  2210]\n",
      " [ 1104 23616]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.77      0.66      0.71      3846\n",
      "        1.0       0.90      0.94      0.92     12435\n",
      "\n",
      "avg / total       0.87      0.87      0.87     16281\n",
      "\n",
      "[[ 2533  1313]\n",
      " [  761 11674]]\n",
      "\n",
      "\n",
      "[ 0.86917266  0.86760641  0.86851562]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "GradientBoostingClassifier\n",
    "\n",
    "\n",
    "\n",
    "parmas:\n",
    "    n_estimators : int (default=100)\n",
    "    The number of boosting stages to perform. Gradient boosting is **fairly robust to over-fitting** so a large number usually results in better performance.\n",
    "\n",
    "'''\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "model = GradientBoostingClassifier(n_estimators=2000, \n",
    "                                   learning_rate=0.1,\n",
    "                                   max_features='sqrt',\n",
    "#                                     max_depth=1,\n",
    "                                   random_state=0)\n",
    "\n",
    "\n",
    "testModelOnData(model, X, y, TX, Ty)\n",
    "# testModelOnData(model, extraTreesX, y, extraTreesTX, Ty)\n",
    "# testModelOnData(model, extraTreesScaledX, y, extraTreesScaledTX, Ty)\n",
    "# testModelOnData(model, extraTreeScaledAndOneHotX, y, extraTreeScaledAndOneHotTX, Ty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data_preprocessing import logisticRegression,kNeighborsDemo,cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=5, error_score='raise',\n",
      "       estimator=VotingClassifier(estimators=[('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)), ('rf', RandomFore...owski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'))],\n",
      "         n_jobs=1, voting='hard', weights=None),\n",
      "       fit_params={}, iid=True, n_jobs=-1,\n",
      "       param_grid={'rf__n_estimators': [20, 200], 'lr__C': [1.0, 100.0]},\n",
      "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
      "       scoring=None, verbose=0)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.94      0.49      0.65      7841\n",
      "        1.0       0.86      0.99      0.92     24720\n",
      "\n",
      "avg / total       0.88      0.87      0.85     32561\n",
      "\n",
      "[[ 3862  3979]\n",
      " [  256 24464]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.81      0.32      0.46      3846\n",
      "        1.0       0.82      0.98      0.89     12435\n",
      "\n",
      "avg / total       0.82      0.82      0.79     16281\n",
      "\n",
      "[[ 1222  2624]\n",
      " [  278 12157]]\n",
      "\n",
      "\n",
      "[ 0.81914502  0.8182237   0.81774625]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Combine method\n",
    "\n",
    "And tuning.\n",
    "\n",
    "'''\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = KNeighborsClassifier()\n",
    "\n",
    "model = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('knn', clf3)], voting='hard')\n",
    "\n",
    "'''\n",
    "[ 0.79767828  0.79887599  0.79839676]\n",
    "[ 0.84116455  0.84798231  0.84769188]\n",
    "[ 0.77833057  0.7740925   0.7734267 ]\n",
    "[ 0.81730238  0.81776304  0.81793053]\n",
    "'''\n",
    "# for clf, label in zip([clf1, clf2, clf3, model], ['Logistic Regression', 'Random Forest', 'Knn', 'Ensemble']):\n",
    "#     scores = cross_validation(clf, X, y)\n",
    "#     print(scores)\n",
    "    \n",
    "\n",
    "    \n",
    "'''\n",
    "\n",
    "'''        \n",
    "# for clf, label in zip([clf1, clf2, clf3, model], ['Logistic Regression', 'Random Forest', 'Knn', 'Ensemble']):\n",
    "#     scores = cross_validation(clf, pcaScaledAndOneHotX, y)\n",
    "#     print(scores)\n",
    "\n",
    "# testModelOnData(model, X, y, TX, Ty)\n",
    "# testModelOnData(model, extraTreesX, y, extraTreesTX, Ty)\n",
    "# testModelOnData(model, extraTreesScaledX, y, extraTreesScaledTX, Ty)\n",
    "# testModelOnData(model, extraTreeScaledAndOneHotX, y, extraTreeScaledAndOneHotTX, Ty)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "[3.2. Tuning the hyper-parameters of an estimator](http://scikit-learn.org/stable/modules/grid_search.html#tuning-the-hyper-parameters-of-an-estimator)\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {'lr__C': [1.0, 100.0], 'rf__n_estimators': [20, 200],}\n",
    "grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5,\n",
    "                   n_jobs=-1)\n",
    "testModelOnData(grid, X, y, TX, Ty)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
