{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCI Adult Dataset Process Demo.\n",
    "Only demonstrate the basic useage of this dataset and some algorithms in scikit library(And most copy with minimum changes from [【机器学习实验】scikit-learn的主要模块和基本使用](http://www.jianshu.com/p/1c6efdbce226)), if we want to have better and more meaningful results,we should do some more work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the preprocessing: transform description to number([male,female]->[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadXandY(fileName):\n",
    "    import numpy as np\n",
    "    \n",
    "    # load the CSV file as a numpy matrix\n",
    "    with open(fileName, 'r') as f:\n",
    "        dataset = np.loadtxt(f, delimiter=\",\")\n",
    "    \n",
    "    # separate the data from the target attributes\n",
    "    attrsize = len(dataset[0])\n",
    "    \n",
    "    print(len(dataset), attrsize)\n",
    "    \n",
    "    X = dataset[:,0:-1]\n",
    "    y = dataset[:,-1]\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32561 15\n",
      "16281 15\n"
     ]
    }
   ],
   "source": [
    "baseDir = 'H:/practice/scikit_class/scikit_learning/uci_adult/adult_data/'\n",
    "# baseDir = 'adult_data/'\n",
    "\n",
    "fileName = baseDir+'adult.data.num'\n",
    "\n",
    "testFileName = baseDir+'adult.test.num'\n",
    "\n",
    "X,y = loadXandY(fileName)\n",
    "TX,Ty = loadXandY(testFileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deal with the missing value\n",
    "[Sci4.3.4. Encoding categorical features](http://scikit-learn.org/stable/modules/preprocessing.html#imputation-of-missing-values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simpleImputer(X):\n",
    "    from sklearn.preprocessing import Imputer\n",
    "    imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "    imp.fit(X)\n",
    "    return imp.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = simpleImputer(X)\n",
    "TX = simpleImputer(TX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The doc mentioned that we should use one hot encoding since `these expect continuous input, and would interpret the categories as being ordered, which is often not desired `.\n",
    "\n",
    "See [scikit-learn机器学习RandomForest实例（含类别属性处理）](http://blog.csdn.net/mach_learn/article/details/40428297)\n",
    "and [Note on using OneHotEncoder in scikit-learn to work on categorical features](https://xgdgsc.wordpress.com/2015/03/20/note-on-using-onehotencoder-in-scikit-learn-to-work-on-categorical-features/) for more info.\n",
    "\n",
    "And our feature is:\n",
    "![features](pics/describe.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def oneHotEncoderX(X):\n",
    "    import numpy as np\n",
    "    from sklearn import preprocessing\n",
    "    to_change_features = list(map(lambda x:x-1, [2,4,6,7,8,9,10,14]))\n",
    "#     enc = preprocessing.OneHotEncoder(categorical_features=to_change_features) # if not specify the n_values, by default, test set will be 104 long\n",
    "    enc = preprocessing.OneHotEncoder(categorical_features=to_change_features, n_values=[8,16,7,14,6,5,2,41])\n",
    "    enc.fit(X)\n",
    "    return np.array(enc.transform(X).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  5.00000000e+01   1.00000000e+00   8.33110000e+04   0.00000000e+00\n",
      "   1.30000000e+01   0.00000000e+00   4.00000000e+00   2.00000000e+00\n",
      "   0.00000000e+00   1.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.30000000e+01   0.00000000e+00]\n",
      "[  0.00000000e+00   1.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   1.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   5.00000000e+01\n",
      "   8.33110000e+04   1.30000000e+01   0.00000000e+00   0.00000000e+00\n",
      "   1.30000000e+01]\n",
      "105\n",
      "105\n"
     ]
    }
   ],
   "source": [
    "X_oneHot = oneHotEncoderX(X)\n",
    "TX_oneHot = oneHotEncoderX(TX)\n",
    "print(X[1])\n",
    "print(X_oneHot[1])\n",
    "print(len(X_oneHot[1]))\n",
    "print(len(TX_oneHot[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = X_oneHot\n",
    "TX = TX_oneHot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据归一化(Data Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simpleScale(X):\n",
    "    from sklearn import preprocessing\n",
    "    # normalize the data attributes\n",
    "    normalized_X = preprocessing.normalize(X)\n",
    "    # standardize the data attributes\n",
    "    standardized_X = preprocessing.scale(X)\n",
    "    return standardized_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = simpleScale(X)\n",
    "TX = simpleScale(TX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征选择(Feature Selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.17274419  0.04627868  0.16203893  0.03643286  0.08773375  0.0700939\n",
      "  0.08065895  0.06733442  0.01465805  0.03654993  0.08149967  0.02940447\n",
      "  0.09472681  0.01984539]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X, y)\n",
    "# display the relative importance of each attribute\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_predictions(y, predicted):\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(y, predicted)\n",
    "    ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
    "    ax.set_xlabel('Measured')\n",
    "    ax.set_ylabel('Predicted')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_predictions(model, X, y):\n",
    "    from sklearn import metrics\n",
    "    # make predictions\n",
    "    expected = y\n",
    "    predicted = model.predict(X)\n",
    "    # summarize the fit of the model\n",
    "    print(metrics.classification_report(expected, predicted))\n",
    "    print(metrics.confusion_matrix(expected, predicted))\n",
    "    print('\\n')\n",
    "#     show_predictions(expected[::len(expected)//20], predicted[::len(predicted)//20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cross_validation(model, X, y):\n",
    "    from sklearn.model_selection import KFold, cross_val_score\n",
    "    k_fold = KFold(n_splits=3)\n",
    "    return cross_val_score(model, X, y, cv=k_fold, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.72      0.54      0.62      3846\n",
      "        1.0       0.87      0.94      0.90     12435\n",
      "\n",
      "avg / total       0.83      0.84      0.83     16281\n",
      "\n",
      "[[ 2094  1752]\n",
      " [  805 11630]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 逻辑回归\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "print(model)\n",
    "\n",
    "# make_predictions(model, X, y)\n",
    "\n",
    "make_predictions(model, TX, Ty)\n",
    "\n",
    "# print(cross_validation(model, X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.69      0.40      0.51      7841\n",
      "        1.0       0.83      0.94      0.88     24720\n",
      "\n",
      "avg / total       0.80      0.81      0.79     32561\n",
      "\n",
      "[[ 3160  4681]\n",
      " [ 1407 23313]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.69      0.41      0.51      3846\n",
      "        1.0       0.84      0.94      0.89     12435\n",
      "\n",
      "avg / total       0.80      0.82      0.80     16281\n",
      "\n",
      "[[ 1567  2279]\n",
      " [  707 11728]]\n",
      "\n",
      "\n",
      "[ 0.8115902   0.81241938  0.81608772]\n"
     ]
    }
   ],
   "source": [
    "# 朴素贝叶斯\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(X, y)\n",
    "print(model)\n",
    "\n",
    "make_predictions(model, X, y)\n",
    "\n",
    "make_predictions(model, TX, Ty)\n",
    "\n",
    "print(cross_validation(model, X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.78      0.69      0.74      7841\n",
      "        1.0       0.91      0.94      0.92     24720\n",
      "\n",
      "avg / total       0.88      0.88      0.88     32561\n",
      "\n",
      "[[ 5437  2404]\n",
      " [ 1504 23216]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.66      0.58      0.62      3846\n",
      "        1.0       0.88      0.91      0.89     12435\n",
      "\n",
      "avg / total       0.83      0.83      0.83     16281\n",
      "\n",
      "[[ 2241  1605]\n",
      " [ 1134 11301]]\n",
      "\n",
      "\n",
      "[ 0.82623917  0.8291874   0.83331798]\n"
     ]
    }
   ],
   "source": [
    "# K近邻\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# fit a k-nearest neighbor model to the data\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(X, y)\n",
    "print(model)\n",
    "\n",
    "make_predictions(model, X, y)\n",
    "\n",
    "make_predictions(model, TX, Ty)\n",
    "\n",
    "print(cross_validation(model, X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 决策树\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# fit a CART model to the data\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X, y)\n",
    "print(model)\n",
    "\n",
    "make_predictions(model, X, y)\n",
    "\n",
    "make_predictions(model, TX, Ty)\n",
    "\n",
    "\n",
    "print(cross_validation(model, X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This block is still about `决策树`\n",
    "\n",
    "# The train set have only one outlier, so it's clearly overfit(since the test dataset is not so appealing),\n",
    "# we want to have this outlier's info,since it can be interesting\n",
    "import numpy as np\n",
    "expected = y\n",
    "predicted = model.predict(X)\n",
    "diff = expected - predicted\n",
    "diff_location = list(np.where(diff != 0)[0])\n",
    "print(len(diff_location), diff_location)\n",
    "\n",
    "def getLine(fileName, lineNum):\n",
    "    with open(fileName, 'r') as f:\n",
    "        all = f.readlines()\n",
    "    \n",
    "    return all[lineNum].strip()\n",
    "\n",
    "getLine(baseDir+'adult.data', 848)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SVM\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "# fit a SVM model to the data\n",
    "model = SVC()\n",
    "model.fit(X, y)\n",
    "print(model)\n",
    "\n",
    "\n",
    "make_predictions(model, X, y)\n",
    "\n",
    "make_predictions(model, TX, Ty)\n",
    "\n",
    "print(cross_validation(model, X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\py_env\\scikit_sys_site_packages\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "H:\\py_env\\scikit_sys_site_packages\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001),\n",
      "       fit_params={}, iid=True, n_jobs=1,\n",
      "       param_grid={'alpha': array([  1.00000e+00,   1.00000e-01,   1.00000e-02,   1.00000e-03,\n",
      "         1.00000e-04,   0.00000e+00])},\n",
      "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)\n",
      "0.31229463325545104\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# 优化算法参数\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "# prepare a range of alpha values to test\n",
    "alphas = np.array([1,0.1,0.01,0.001,0.0001,0])\n",
    "# create and fit a ridge regression model, testing each alpha\n",
    "model = Ridge()\n",
    "grid = GridSearchCV(estimator=model, param_grid=dict(alpha=alphas))\n",
    "grid.fit(X, y)\n",
    "print(grid)\n",
    "# summarize the results of the grid search\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV(cv=None, error_score='raise',\n",
      "          estimator=Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001),\n",
      "          fit_params={}, iid=True, n_iter=100, n_jobs=1,\n",
      "          param_distributions={'alpha': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000001A213A70400>},\n",
      "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
      "          scoring=None, verbose=0)\n",
      "0.31229463315387784\n",
      "0.996920839648\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import uniform as sp_rand\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "# prepare a uniform distribution to sample for the alpha parameter\n",
    "param_grid = {'alpha': sp_rand()}\n",
    "# create and fit a ridge regression model, testing random alpha values\n",
    "model = Ridge()\n",
    "rsearch = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=100)\n",
    "rsearch.fit(X, y)\n",
    "print(rsearch)\n",
    "# summarize the results of the random parameter search\n",
    "print(rsearch.best_score_)\n",
    "print(rsearch.best_estimator_.alpha)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}