{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCI Adult Dataset Process Demo.\n",
    "Only demonstrate the basic useage of this dataset and some algorithms in scikit library(And most copy with minimum changes from [【机器学习实验】scikit-learn的主要模块和基本使用](http://www.jianshu.com/p/1c6efdbce226)), if we want to have better and more meaningful results,we should do some more work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the preprocessing: transform description to number([male,female]->[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadXandY(fileName):\n",
    "    import numpy as np\n",
    "    \n",
    "    # load the CSV file as a numpy matrix\n",
    "    with open(fileName, 'r') as f:\n",
    "        dataset = np.loadtxt(f, delimiter=\",\")\n",
    "    \n",
    "    # separate the data from the target attributes\n",
    "    attrsize = len(dataset[0])\n",
    "    \n",
    "    print(len(dataset), attrsize)\n",
    "    \n",
    "    X = dataset[:,0:-1]\n",
    "    y = dataset[:,-1]\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32561 15\n",
      "16281 15\n"
     ]
    }
   ],
   "source": [
    "baseDir = 'H:/practice/scikit_class/scikit_learning/uci_adult/adult_data/'\n",
    "# baseDir = 'adult_data/'\n",
    "\n",
    "fileName = baseDir+'adult.data.num'\n",
    "\n",
    "testFileName = baseDir+'adult.test.num'\n",
    "\n",
    "X,y = loadXandY(fileName)\n",
    "TX,Ty = loadXandY(testFileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deal with the missing value\n",
    "[reference](http://scikit-learn.org/stable/modules/preprocessing.html#imputation-of-missing-values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simpleImputer(X):\n",
    "    from sklearn.preprocessing import Imputer\n",
    "    imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "    imp.fit(X)\n",
    "    return imp.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = simpleImputer(X)\n",
    "TX = simpleImputer(TX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据归一化(Data Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simpleProcessing(X):\n",
    "    from sklearn import preprocessing\n",
    "    # normalize the data attributes\n",
    "    normalized_X = preprocessing.normalize(X)\n",
    "    # standardize the data attributes\n",
    "    standardized_X = preprocessing.scale(X)\n",
    "    return standardized_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = simpleProcessing(X)\n",
    "TX = simpleProcessing(TX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征选择(Feature Selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.16746732  0.04252195  0.16567563  0.03698216  0.08049178  0.10211605\n",
      "  0.07715989  0.04817133  0.01456404  0.03066858  0.09105681  0.03045428\n",
      "  0.09249326  0.02017692]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X, y)\n",
    "# display the relative importance of each attribute\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_predictions(y, predicted):\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(y, predicted)\n",
    "    ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
    "    ax.set_xlabel('Measured')\n",
    "    ax.set_ylabel('Predicted')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_predictions(model, X, y):\n",
    "    from sklearn import metrics\n",
    "    # make predictions\n",
    "    expected = y\n",
    "    predicted = model.predict(X)\n",
    "    # summarize the fit of the model\n",
    "    print(metrics.classification_report(expected, predicted))\n",
    "    print(metrics.confusion_matrix(expected, predicted))\n",
    "    print('\\n')\n",
    "#     show_predictions(expected[::len(expected)//20], predicted[::len(predicted)//20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cross_validation(model, X, y):\n",
    "    from sklearn.model_selection import KFold, cross_val_score\n",
    "    k_fold = KFold(n_splits=3)\n",
    "    return cross_val_score(model, X, y, cv=k_fold, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.73      0.55      0.63      7841\n",
      "        1.0       0.87      0.93      0.90     24720\n",
      "\n",
      "avg / total       0.83      0.84      0.83     32561\n",
      "\n",
      "[[ 4304  3537]\n",
      " [ 1609 23111]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.72      0.54      0.62      3846\n",
      "        1.0       0.87      0.94      0.90     12435\n",
      "\n",
      "avg / total       0.83      0.84      0.83     16281\n",
      "\n",
      "[[ 2094  1752]\n",
      " [  805 11630]]\n",
      "\n",
      "\n",
      "[ 0.83876912  0.84116455  0.84391413]\n"
     ]
    }
   ],
   "source": [
    "# 逻辑回归\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "print(model)\n",
    "\n",
    "make_predictions(model, X, y)\n",
    "\n",
    "make_predictions(model, TX, Ty)\n",
    "\n",
    "print(cross_validation(model, X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.69      0.40      0.51      7841\n",
      "        1.0       0.83      0.94      0.88     24720\n",
      "\n",
      "avg / total       0.80      0.81      0.79     32561\n",
      "\n",
      "[[ 3160  4681]\n",
      " [ 1407 23313]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.69      0.41      0.51      3846\n",
      "        1.0       0.84      0.94      0.89     12435\n",
      "\n",
      "avg / total       0.80      0.82      0.80     16281\n",
      "\n",
      "[[ 1567  2279]\n",
      " [  707 11728]]\n",
      "\n",
      "\n",
      "[ 0.8115902   0.81241938  0.81608772]\n"
     ]
    }
   ],
   "source": [
    "# 朴素贝叶斯\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(X, y)\n",
    "print(model)\n",
    "\n",
    "make_predictions(model, X, y)\n",
    "\n",
    "make_predictions(model, TX, Ty)\n",
    "\n",
    "print(cross_validation(model, X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.78      0.69      0.74      7841\n",
      "        1.0       0.91      0.94      0.92     24720\n",
      "\n",
      "avg / total       0.88      0.88      0.88     32561\n",
      "\n",
      "[[ 5437  2404]\n",
      " [ 1504 23216]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.66      0.58      0.62      3846\n",
      "        1.0       0.88      0.91      0.89     12435\n",
      "\n",
      "avg / total       0.83      0.83      0.83     16281\n",
      "\n",
      "[[ 2241  1605]\n",
      " [ 1134 11301]]\n",
      "\n",
      "\n",
      "[ 0.82623917  0.8291874   0.83331798]\n"
     ]
    }
   ],
   "source": [
    "# K近邻\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# fit a k-nearest neighbor model to the data\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(X, y)\n",
    "print(model)\n",
    "\n",
    "make_predictions(model, X, y)\n",
    "\n",
    "make_predictions(model, TX, Ty)\n",
    "\n",
    "print(cross_validation(model, X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      1.00      1.00      7841\n",
      "        1.0       1.00      1.00      1.00     24720\n",
      "\n",
      "avg / total       1.00      1.00      1.00     32561\n",
      "\n",
      "[[ 7841     0]\n",
      " [    1 24719]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.61      0.61      0.61      3846\n",
      "        1.0       0.88      0.88      0.88     12435\n",
      "\n",
      "avg / total       0.81      0.81      0.81     16281\n",
      "\n",
      "[[ 2340  1506]\n",
      " [ 1526 10909]]\n",
      "\n",
      "\n",
      "[ 0.80652294  0.81463055  0.81295494]\n"
     ]
    }
   ],
   "source": [
    "# 决策树\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# fit a CART model to the data\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X, y)\n",
    "print(model)\n",
    "\n",
    "make_predictions(model, X, y)\n",
    "\n",
    "make_predictions(model, TX, Ty)\n",
    "\n",
    "\n",
    "print(cross_validation(model, X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [848]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'39, Private, 138192, Bachelors, 13, Married-civ-spouse, Craft-repair, Husband, White, Male, 0, 0, 40, United-States, <=50K'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This block is still about `决策树`\n",
    "\n",
    "# The train set have only one outlier, so it's clearly overfit(since the test dataset is not so appealing),\n",
    "# we want to have this outlier's info,since it can be interesting\n",
    "import numpy as np\n",
    "expected = y\n",
    "predicted = model.predict(X)\n",
    "diff = expected - predicted\n",
    "diff_location = list(np.where(diff != 0)[0])\n",
    "print(len(diff_location), diff_location)\n",
    "\n",
    "def getLine(fileName, lineNum):\n",
    "    with open(fileName, 'r') as f:\n",
    "        all = f.readlines()\n",
    "    \n",
    "    return all[lineNum].strip()\n",
    "\n",
    "getLine(baseDir+'adult.data', 848)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.78      0.56      0.65      7841\n",
      "        1.0       0.87      0.95      0.91     24720\n",
      "\n",
      "avg / total       0.85      0.85      0.85     32561\n",
      "\n",
      "[[ 4366  3475]\n",
      " [ 1247 23473]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.76      0.54      0.63      3846\n",
      "        1.0       0.87      0.95      0.91     12435\n",
      "\n",
      "avg / total       0.84      0.85      0.84     16281\n",
      "\n",
      "[[ 2089  1757]\n",
      " [  666 11769]]\n",
      "\n",
      "\n",
      "[ 0.84567901  0.85019348  0.84953469]\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "# fit a SVM model to the data\n",
    "model = SVC()\n",
    "model.fit(X, y)\n",
    "print(model)\n",
    "\n",
    "\n",
    "make_predictions(model, X, y)\n",
    "\n",
    "make_predictions(model, TX, Ty)\n",
    "\n",
    "print(cross_validation(model, X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\py_env\\scikit_sys_site_packages\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "H:\\py_env\\scikit_sys_site_packages\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001),\n",
      "       fit_params={}, iid=True, n_jobs=1,\n",
      "       param_grid={'alpha': array([  1.00000e+00,   1.00000e-01,   1.00000e-02,   1.00000e-03,\n",
      "         1.00000e-04,   0.00000e+00])},\n",
      "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)\n",
      "0.31229463325545104\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# 优化算法参数\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "# prepare a range of alpha values to test\n",
    "alphas = np.array([1,0.1,0.01,0.001,0.0001,0])\n",
    "# create and fit a ridge regression model, testing each alpha\n",
    "model = Ridge()\n",
    "grid = GridSearchCV(estimator=model, param_grid=dict(alpha=alphas))\n",
    "grid.fit(X, y)\n",
    "print(grid)\n",
    "# summarize the results of the grid search\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV(cv=None, error_score='raise',\n",
      "          estimator=Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001),\n",
      "          fit_params={}, iid=True, n_iter=100, n_jobs=1,\n",
      "          param_distributions={'alpha': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000001A213A70400>},\n",
      "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
      "          scoring=None, verbose=0)\n",
      "0.31229463315387784\n",
      "0.996920839648\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import uniform as sp_rand\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "# prepare a uniform distribution to sample for the alpha parameter\n",
    "param_grid = {'alpha': sp_rand()}\n",
    "# create and fit a ridge regression model, testing random alpha values\n",
    "model = Ridge()\n",
    "rsearch = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=100)\n",
    "rsearch.fit(X, y)\n",
    "print(rsearch)\n",
    "# summarize the results of the random parameter search\n",
    "print(rsearch.best_score_)\n",
    "print(rsearch.best_estimator_.alpha)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
